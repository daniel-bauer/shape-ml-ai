{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1855f8ac-823b-4216-9ac3-043429ecb58e",
   "metadata": {},
   "source": [
    "## Machine Learning and Artificial Intelligence \n",
    "Summer High School Academic Program for Engineers (2025)\n",
    "## PyTorch Basics\n",
    "\n",
    "torch is an open-source machine library and a scientific computing framework. PyTorch is the python frontend to torch.  \n",
    "\n",
    "PyTorch provides the *tensor* data structure, which is similar to a numpy ndarray, but with added features. Notably, tensor operations can be accellerated on a GPU. Tensors also\n",
    "offer support for automatic differentiation / backpropagation, so we do not have to implement this from scratch. \n",
    "\n",
    "Pytorch comes with a higher-level library to construct neural networks (torch.nn), and provides infrastructure for working with data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d886ef85-986d-4846-94d0-cc30089f7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b69568-1241-49cb-81c2-26401a2020c0",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are used for 1) storing data 2) storing activations (and pre-activations) in a neural network 3) storing parameters.\n",
    "They support most of the same operations as numpy arrays do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc01124-105d-43b6-93b3-1876c0282cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1,2,3])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d3a5f20-f3e8-4a71-a711-906a4d06fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52dcb2ed-830b-435f-af46-1c14f04d9b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3144f2e1-e99d-4405-adcc-758542df5659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "671524ef-28bd-45c9-9049-9579a971f8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0,1] #Note: the result is a 0D tensor = a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "472437dc-bc20-46ed-9511-585c5c4abead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6f2b0c-4c05-4f16-883d-856fed6a70b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32, 50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W @ t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75e0a87d-2421-44d1-b0ac-b90d8b912320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 7],\n",
       "        [2, 5, 8],\n",
       "        [3, 6, 9]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b46b8fba-3b43-487c-a45f-16bd109c62ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6f23e-e404-4326-a180-d3eff96b1913",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "When we create a tensor we can set the parameter `requires_grad=True`. PyTorch will then track operations on these tensors, \n",
    "and automatically construct a computational graph in the background. \n",
    "\n",
    "When we call the `.backward()` method on a tensor, PyTorch traverses this graph in reverse and applies the chain rule to compute gradients automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1097a7cc-ad40-43f1-b547-3b32480b19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative w.r.t a: 4.0, partial derivative w.r.t b: 9.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = (a + b) * (b + 1)  # computation graph built here\n",
    "\n",
    "y.backward()  # compute dy/dx\n",
    "print(f\"partial derivative w.r.t a: {a.grad}, partial derivative w.r.t b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623b00d-50d5-4785-92d9-0e6cd86594d1",
   "metadata": {},
   "source": [
    "We can use this to rebuild the neural network we created from scratch in numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1cd9226c-86ad-45e7-9e96-fb64d107c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.rand(10 ,4, requires_grad = True)\n",
    "b1 = torch.rand(10, requires_grad = True)\n",
    "\n",
    "W2 = torch.rand(10, 10, requires_grad = True)\n",
    "b2 = torch.rand(10, requires_grad = True)\n",
    "\n",
    "W3 = torch.rand(3, 10, requires_grad = True)\n",
    "b3 = torch.rand(3, requires_grad = True)\n",
    "\n",
    "def loss(prediction, y_one_hot):\n",
    "    return -torch.sum(y_one_hot * torch.log(prediction)) \n",
    "\n",
    "def forward(x):\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = torch.sigmoid(z2)\n",
    "    scores = W3 @ a2 + b3    \n",
    "    print(scores)\n",
    "    probs = torch.softmax(scores,-1)\n",
    "    return probs\n",
    "\n",
    "def step(x,y): \n",
    "    probs = forward(x)\n",
    "    loss_val = loss(probs, y)\n",
    "    loss_val.backward()  # THIS DOES THE BACKPROP FOR US :-)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1936f07e-525c-46a7-9319-40300036a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "data_x = torch.tensor([   # 4 attributes \n",
    "    [0.5, -0.2, 0.1, 0.4],\n",
    "    [1.5,  0.2, 1.1, -0.4],\n",
    "    [0.3,  0.8, 0.5, 0.7],\n",
    "    [0.6,  0.3, -0.9, 1.0],\n",
    "    [1.0, -0.1, 0.2, -0.3]\n",
    "])\n",
    "\n",
    "# There are 3 classes 0, 1, 2.\n",
    "data_y = torch.tensor([0, 2, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "265717fb-d344-42b6-9e34-81d937b69007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.2363, 4.5511, 6.0235], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y): \n",
    "    return torch.nn.functional.one_hot(y, num_classes=3)\n",
    "\n",
    "step(data_x[0], one_hot(data_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc8579-9af9-4f3f-bdae-0a269208849c",
   "metadata": {},
   "source": [
    "### The Module Abstraction \n",
    "\n",
    "The `torch.nn` component contains a number of tools to build neural networks more easily. Rather than building neural networks from scratch using individual tensors, which can become very cumbersome for larger networks, we can construct the network from `Modules`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "26125349-7ce3-4fe5-a090-da909cfc253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # common convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6200b86-eeb2-483a-9540-d718d6839f2e",
   "metadata": {},
   "source": [
    "`nn.Module` is the base class for all other Modules. We can get a standard neural network layer using the `nn.Sequential` Module. \n",
    "\n",
    "Each Module has a `.forward(x)` method that returns the activation of the module when applied to the tensor `x`. Alternatively, the module can just be called *as if it was a function*, which will implicitly call `.forward`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "252a7a0e-f3b5-4aa2-9cb8-a99904b97a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4205,  0.3464, -0.2782], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(4, 3)  #linear layer without an activation function.  \n",
    "test_input = data_x[0]\n",
    "linear(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d69e1923-5333-43e0-a252-3baf8b22c6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4205,  0.3464, -0.2782], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.forward(test_input) # equivalent, but directly calling the module is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070ecb9-6b28-4c97-828a-81c2dc20106b",
   "metadata": {},
   "source": [
    "Each module keeps track of its parameters (weights and biases). All of these have `requires_grad=True` set by default. But the beauty of the module abstraction is that we rarely have to look at these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e744f07d-bbd9-4496-a1fa-10ad143b5f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3690,  0.4537,  0.1393, -0.1728],\n",
      "        [ 0.2088,  0.3057,  0.3565, -0.1958],\n",
      "        [-0.1450,  0.1597, -0.1829,  0.0631]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4591,  0.3458, -0.1807], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters(): \n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c89c41-c1ab-4751-9ba4-15bfe3d540e3",
   "metadata": {},
   "source": [
    "**Writing your own Modules**\n",
    "\n",
    "Modules can be nested inside of other modules -- this is fairly common in larger neural networks, and also the way you typically write your own neural network in pyTorch.\n",
    "\n",
    "The basic approach is to inherit from nn.Module (review the notes on inheritance in the Python review section -- but really all this means is that the methods of nn.Module become available in your custom module). We add all component modules in the `__init__` method, which is called when the new class is first instantiated. Then we write a `forward` method to define the computation flow through the network. autograd will construct the computation graph for us in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "96adc306-f94d-46b5-b3a0-4e73eab6d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F # anoter common convention\n",
    "\n",
    "class Model(nn.Module): # inherit from the nn.Module class\n",
    "\n",
    "    def __init__(self): \n",
    "        # This is the constructor. \n",
    "        super().__init__() # call constructor of the parent class\n",
    "        \n",
    "        # Specify all the component modules inside of __init__. \n",
    "        self.hidden1 = nn.Linear(4,10)\n",
    "        self.hidden2 = nn.Linear(10,10)\n",
    "        self.out_layer = nn.Linear(10,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Need to implement the forward method to specify computation flow\n",
    "        z1 = self.hidden1(x)\n",
    "        a1 = F.relu(z1)\n",
    "        z2 = self.hidden2(a1)\n",
    "        a2 = F.relu(z2) \n",
    "        z3 = self.out_layer(a2) \n",
    "        return z3 \n",
    "\n",
    "\n",
    "# Note: We are NOT applying softmax here. Below, we will use pyTorch's CategoricalCrossentropy loss, which implicitly computes the softmax \n",
    "# before comparing to the target one-hot vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "f7387430-8292-4c5e-b198-6f0201f2ca03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2202,  0.3002, -0.0720], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(data_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e01119-912b-4e9c-8f9a-9c2e56471f8b",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We can now write a training loop, either for our hand-built neural network or for the the Module version.  \n",
    "We will use <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">nn.CrossEntropyLoss</a>. This implementation allows us to compare the prediction directly to an integer class. So the target can just be an an integer scalar, not a one-hot vector. CrossEntropyLoss also computes the softmax operation implicitly before comparing to the target. \n",
    "\n",
    "PyTorch provides a number of differen optimization algorithms. Here we will just use the basic Stochastic Gradient Descent algorithm we already know. But more advanced algorithms are available in the <a href=\"https://docs.pytorch.org/docs/stable/optim.html\">torch.optim</a> package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "be09cab2-3a25-409d-a5f2-51a36867c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_x, data_y, epochs=100, learning_rate=0.1):\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "    \n",
    "    #register the model's parameters with the optimizer, so it knows what to update. \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "    model.train() # Put the model in training mode. This doesn't hve any effect for now, but it's good practice. \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i in range(len(data_x)):\n",
    "            x = data_x[i]                \n",
    "            y = data_y[i]\n",
    "\n",
    "            prediction = model(x)            # raw class scores            \n",
    "\n",
    "            loss = loss_fn(prediction, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "be11831a-d37b-4797-bd39-1c3e826c1c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 5.398195028305054\n",
      "Epoch 2: Loss = 5.267715513706207\n",
      "Epoch 3: Loss = 5.1859389543533325\n",
      "Epoch 4: Loss = 5.046169936656952\n",
      "Epoch 5: Loss = 4.857535719871521\n",
      "Epoch 6: Loss = 4.721572637557983\n",
      "Epoch 7: Loss = 4.468655467033386\n",
      "Epoch 8: Loss = 4.21475487947464\n",
      "Epoch 9: Loss = 4.056803077459335\n",
      "Epoch 10: Loss = 3.7503445148468018\n",
      "Epoch 11: Loss = 3.5339067578315735\n",
      "Epoch 12: Loss = 3.381823390722275\n",
      "Epoch 13: Loss = 3.214255303144455\n",
      "Epoch 14: Loss = 3.069305196404457\n",
      "Epoch 15: Loss = 2.9365364760160446\n",
      "Epoch 16: Loss = 2.8356797993183136\n",
      "Epoch 17: Loss = 2.694264739751816\n",
      "Epoch 18: Loss = 2.6512050330638885\n",
      "Epoch 19: Loss = 2.55254065990448\n",
      "Epoch 20: Loss = 2.475143790245056\n",
      "Epoch 21: Loss = 2.368068002164364\n",
      "Epoch 22: Loss = 2.3232356309890747\n",
      "Epoch 23: Loss = 2.2044807337224483\n",
      "Epoch 24: Loss = 2.1476596482098103\n",
      "Epoch 25: Loss = 2.0254349149763584\n",
      "Epoch 26: Loss = 1.9394690729677677\n",
      "Epoch 27: Loss = 1.8480589017271996\n",
      "Epoch 28: Loss = 1.6801892146468163\n",
      "Epoch 29: Loss = 1.5095825120806694\n",
      "Epoch 30: Loss = 1.5752169005572796\n",
      "Epoch 31: Loss = 1.3687578458338976\n",
      "Epoch 32: Loss = 1.181170916184783\n",
      "Epoch 33: Loss = 0.98435128480196\n",
      "Epoch 34: Loss = 0.9995519444346428\n",
      "Epoch 35: Loss = 0.7888172529637814\n",
      "Epoch 36: Loss = 0.5876045394688845\n",
      "Epoch 37: Loss = 0.5793837606906891\n",
      "Epoch 38: Loss = 0.40800842083990574\n",
      "Epoch 39: Loss = 0.4131601797416806\n",
      "Epoch 40: Loss = 0.27878252789378166\n",
      "Epoch 41: Loss = 0.2820217637345195\n",
      "Epoch 42: Loss = 0.23013862129300833\n",
      "Epoch 43: Loss = 0.19104112777858973\n",
      "Epoch 44: Loss = 0.18476420640945435\n",
      "Epoch 45: Loss = 0.15854824893176556\n",
      "Epoch 46: Loss = 0.14203270990401506\n",
      "Epoch 47: Loss = 0.13731070375069976\n",
      "Epoch 48: Loss = 0.12231422821059823\n",
      "Epoch 49: Loss = 0.10935354558750987\n",
      "Epoch 50: Loss = 0.10798264527693391\n",
      "Epoch 51: Loss = 0.0984711293131113\n",
      "Epoch 52: Loss = 0.08914496935904026\n",
      "Epoch 53: Loss = 0.08893147110939026\n",
      "Epoch 54: Loss = 0.0805608737282455\n",
      "Epoch 55: Loss = 0.07742513017728925\n",
      "Epoch 56: Loss = 0.07102541252970695\n",
      "Epoch 57: Loss = 0.07062382576987147\n",
      "Epoch 58: Loss = 0.0654725399799645\n",
      "Epoch 59: Loss = 0.06268468452617526\n",
      "Epoch 60: Loss = 0.058557290118187666\n",
      "Epoch 61: Loss = 0.05775543535128236\n",
      "Epoch 62: Loss = 0.054693565936759114\n",
      "Epoch 63: Loss = 0.0524884716141969\n",
      "Epoch 64: Loss = 0.049059143057093024\n",
      "Epoch 65: Loss = 0.049170899437740445\n",
      "Epoch 66: Loss = 0.04646124877035618\n",
      "Epoch 67: Loss = 0.044574492843821645\n",
      "Epoch 68: Loss = 0.04325293772853911\n",
      "Epoch 69: Loss = 0.04127786518074572\n",
      "Epoch 70: Loss = 0.040593469981104136\n",
      "Epoch 71: Loss = 0.03875902434810996\n",
      "Epoch 72: Loss = 0.03729545511305332\n",
      "Epoch 73: Loss = 0.036647003376856446\n",
      "Epoch 74: Loss = 0.03555637178942561\n",
      "Epoch 75: Loss = 0.03405613289214671\n",
      "Epoch 76: Loss = 0.03344197780825198\n",
      "Epoch 77: Loss = 0.03199520613998175\n",
      "Epoch 78: Loss = 0.031631543301045895\n",
      "Epoch 79: Loss = 0.030598494922742248\n",
      "Epoch 80: Loss = 0.029365630820393562\n",
      "Epoch 81: Loss = 0.029153312789276242\n",
      "Epoch 82: Loss = 0.028169112280011177\n",
      "Epoch 83: Loss = 0.027594578452408314\n",
      "Epoch 84: Loss = 0.026825444656424224\n",
      "Epoch 85: Loss = 0.025972683215513825\n",
      "Epoch 86: Loss = 0.02562478499021381\n",
      "Epoch 87: Loss = 0.02503120037727058\n",
      "Epoch 88: Loss = 0.024266055901534855\n",
      "Epoch 89: Loss = 0.02388245251495391\n",
      "Epoch 90: Loss = 0.0231462117517367\n",
      "Epoch 91: Loss = 0.02286340098362416\n",
      "Epoch 92: Loss = 0.022323360317386687\n",
      "Epoch 93: Loss = 0.021563872112892568\n",
      "Epoch 94: Loss = 0.021518214256502688\n",
      "Epoch 95: Loss = 0.02098810113966465\n",
      "Epoch 96: Loss = 0.020520123653113842\n",
      "Epoch 97: Loss = 0.02002983132842928\n",
      "Epoch 98: Loss = 0.01976683852262795\n",
      "Epoch 99: Loss = 0.019234829698689282\n",
      "Epoch 100: Loss = 0.01903198752552271\n"
     ]
    }
   ],
   "source": [
    "train(model, data_x, data_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ea05b-7320-41c0-885f-4af90413c27d",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "We can write another function to evaluate the model on some test data. \n",
    "\n",
    "Note that we are turning off gradient computation here, because otherwise pyTorch would automatically store all activations during the forward pass and compute the gradients. Turning this off saves time and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "a4e1bb50-f680-40af-aa7a-dd5fa7b2ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_x, data_y):\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()  # place the model in evaluation mode -- again this doesn't have an effect for now, but is good practice. \n",
    "    \n",
    "    with torch.no_grad():  # turn off automatic gradient computation and storage -- not needed since we are no longer training. \n",
    "        for i in range(data_x.shape[0]):\n",
    "            x = data_x[i]\n",
    "            y = data_y[i]\n",
    "\n",
    "            prediction = model(x)                 # raw class scores. We are just interested in the max, so no need to use softmax.\n",
    "            predicted = torch.argmax(prediction)  # predicted class index -- the one with the max score.\n",
    "\n",
    "            if predicted == y:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / data_x.shape[0]\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "d76c8601-2f7f-4e03-93f7-92c4e76117a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, data_x, data_y) # Run it on the training data for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374419f9-6356-425d-b282-de437909e932",
   "metadata": {},
   "source": [
    "### Datasets and Batching\n",
    "\n",
    "\n",
    "***Mini Batches***\n",
    "So far, we have trained the model on a single input/output pair at a time: present the input, compute the loss, compute the gradients and update the weights. This process can be slow with large amounts of training data, and it also tends to be unstable as a single example can drastically change the weights. One important trick is to shuffle the data so in each epoch they are presented in different order (the \"stochastic\" in SGD). \n",
    "\n",
    "Alternatively, we could use the entire available training data for the forward pass, average the losses (to compute the training error) and compute the gradients for this aggregate error. But with large datasets this can result in overfitting and can be quite memory intensive.\n",
    "\n",
    "As a compromise, we often present the data in \"mini batches\": perform the forward pass on a few data items at a time, average the losses for those items, then compute the gradient. This is especially useful on GPUs, where we can efficiently parallelize tensor operations and \"stack\" together the forward and backward computation for multiple data items at the same time. \n",
    "\n",
    "***Dataset and Dataloader***\n",
    "pyTorch provides a mechanism for maintaining data sets and automatically creating batches for training. \n",
    "\n",
    "The class `torch.utils.data.Dataset` is a base class that can be implemented by various concrete Dataset implementations. It has two methods: \n",
    "\n",
    "* __len__(self) — returns the number of samples in the dataset. Enables the `len(data)` method.\n",
    "* __getitem__(self, k) — returns an (input, output) tuple for the data at index k. Enables indexing, such as `data[5]`.\n",
    "\n",
    "We can build our own Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "671224b4-30c3-408c-81ac-624d425f7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummyData(Dataset): \n",
    "\n",
    "    def __init__(self):\n",
    "        # Possibly load the data from a file and store it in a data structure. Here we will just use the dummy data from above\n",
    "        # Dummy data\n",
    "        self.data_x = torch.tensor([   # 4 attributes \n",
    "            [0.5, -0.2, 0.1, 0.4],\n",
    "            [1.5,  0.2, 1.1, -0.4],\n",
    "            [0.3,  0.8, 0.5, 0.7],\n",
    "            [0.6,  0.3, -0.9, 1.0],\n",
    "            [1.0, -0.1, 0.2, -0.3]])\n",
    "        \n",
    "        self.data_y = torch.tensor([0, 2, 1, 1, 0])\n",
    "        \n",
    "    \n",
    "    def __len__(self): \n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self,k): \n",
    "        return (self.data_x[k], self.data_y[k])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "cd6b5fbd-c034-487c-a9f0-e72f01ee25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DummyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "68bf7b03-ec0c-4126-8662-78c6b4ecfa04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "d8bde565-f98d-4a4b-946c-134884fa6bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0000, -0.1000,  0.2000, -0.3000]), tensor(0))"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d904e5-4618-4599-a16d-f8f9c481f804",
   "metadata": {},
   "source": [
    "The DataLoader will automatically batch the data and allow us to obtain one item at a time for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "e9c3c3e8-5580-435b-a150-372aaa82cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "18812485-2885-4036-bdba-aef127e74f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1.5000,  0.2000,  1.1000, -0.4000],\n",
      "        [ 0.3000,  0.8000,  0.5000,  0.7000]]), tensor([2, 1])]\n",
      "[tensor([[ 1.0000, -0.1000,  0.2000, -0.3000],\n",
      "        [ 0.5000, -0.2000,  0.1000,  0.4000]]), tensor([0, 0])]\n",
      "[tensor([[ 0.6000,  0.3000, -0.9000,  1.0000]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for batch in loader: \n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77754368-40ba-4853-946e-6552ad22a8ce",
   "metadata": {},
   "source": [
    "If the data is in a tensor, PyTorch actually provides an existing TensorDataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "059dfaee-94a8-4bb0-a6d4-176aa5ab281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "data = TensorDataset(data_x, data_y)\n",
    "loader = DataLoader(data, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "1557b143-0822-45dc-a7af-080eee11de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.6000,  0.3000, -0.9000,  1.0000],\n",
      "        [ 0.3000,  0.8000,  0.5000,  0.7000]]), tensor([1, 1])]\n",
      "[tensor([[ 0.5000, -0.2000,  0.1000,  0.4000],\n",
      "        [ 1.0000, -0.1000,  0.2000, -0.3000]]), tensor([0, 0])]\n",
      "[tensor([[ 1.5000,  0.2000,  1.1000, -0.4000]]), tensor([2])]\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d612d8-d34d-4a3b-a83a-56130436b802",
   "metadata": {},
   "source": [
    "Now update the train method to use the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "efb95f4f-96c5-4ddb-96c4-a2fa84101479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model() # Reset the model\n",
    "\n",
    "def train(model, dataloader, epochs=100, learning_rate=0.1):\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "    \n",
    "    #register the model's parameters with the optimizer, so it knows what to update. \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "    model.train() # Put the model in training mode. This doesn't hve any effect for now, but it's good practice. \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x,y in dataloader:            \n",
    "\n",
    "            prediction = model(x)            # raw class scores            \n",
    "\n",
    "            loss = loss_fn(prediction, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb4800-96d2-47dd-9aa8-d53f5f2c5aa4",
   "metadata": {},
   "source": [
    "train(model, loader)  # This now uses batches! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c10755-b85c-4198-bd1d-3f5ec306b7de",
   "metadata": {},
   "source": [
    "### GPU Acceleration \n",
    "\n",
    "Graphics Processing Units (GPUs) are specialized processors originally developed to accelerate graphics rendering and animation, such as those used in video games and CGI for films.\n",
    "\n",
    "GPUs are are designed for parallel processing: They can apply the same operation across many data elements simultaneously. This makes them  well-suited for the kinds of computations common in neural networks, such as element-wise operations and matrix multiplication.\n",
    "\n",
    "PyTorch can used Nvidia's CUDA computing library behind the scenes to accellerate tensor operations. There are also backends for other GPU frameworks, such as Apple's MPS (Metal Performance Shaders) for their Silicon architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377be87-5bc9-4776-a1d6-46693baf825b",
   "metadata": {},
   "source": [
    "First, we can check if CUDA is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "70f647ee-46d2-405b-a933-fafbf29317ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314a807-8abb-467c-b13c-990662c1882c",
   "metadata": {},
   "source": [
    "All we need to do in order for operations to be performed by the GPU is to make sure the relevant tensors (data and weights) are moved to the GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "9af74a56-2178-400f-88bd-f4e9d0e1613a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[506], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad180011-9bba-4ab7-bb25-ee65396dc33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
