{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1855f8ac-823b-4216-9ac3-043429ecb58e",
   "metadata": {},
   "source": [
    "## Machine Learning and Artificial Intelligence \n",
    "Summer High School Academic Program for Engineers (2025)\n",
    "## PyTorch Basics\n",
    "\n",
    "torch is an open-source machine library and a scientific computing framework. PyTorch is the python frontend to torch.  \n",
    "\n",
    "PyTorch provides the *tensor* data structure, which is similar to a numpy ndarray, but with added features. Notably, tensor operations can be accellerated on a GPU. Tensors also\n",
    "offer support for automatic differentiation / backpropagation, so we do not have to implement this from scratch. \n",
    "\n",
    "Pytorch comes with a higher-level library to construct neural networks (torch.nn), and provides infrastructure for working with data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886ef85-986d-4846-94d0-cc30089f7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b69568-1241-49cb-81c2-26401a2020c0",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are used for 1) storing data 2) storing activations (and pre-activations) in a neural network 3) storing parameters.\n",
    "They support most of the same operations as numpy arrays do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc01124-105d-43b6-93b3-1876c0282cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a5f20-f3e8-4a71-a711-906a4d06fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcb2ed-830b-435f-af46-1c14f04d9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144f2e1-e99d-4405-adcc-758542df5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671524ef-28bd-45c9-9049-9579a971f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0,1] #Note: the result is a 0D tensor = a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472437dc-bc20-46ed-9511-585c5c4abead",
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f2b0c-4c05-4f16-883d-856fed6a70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W @ t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0a87d-2421-44d1-b0ac-b90d8b912320",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b8fba-3b43-487c-a45f-16bd109c62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6f23e-e404-4326-a180-d3eff96b1913",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "When we create a tensor we can set the parameter `requires_grad=True`. PyTorch will then track operations on these tensors, \n",
    "and automatically construct a computational graph in the background. \n",
    "\n",
    "When we call the `.backward()` method on a tensor, PyTorch traverses this graph in reverse and applies the chain rule to compute gradients automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097a7cc-ad40-43f1-b547-3b32480b19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = (a + b) * (b + 1)  # computation graph built here\n",
    "\n",
    "y.backward()  # compute dy/dx\n",
    "print(f\"partial derivative w.r.t a: {a.grad}, partial derivative w.r.t b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623b00d-50d5-4785-92d9-0e6cd86594d1",
   "metadata": {},
   "source": [
    "We can use this to rebuild the neural network we created from scratch in numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9226c-86ad-45e7-9e96-fb64d107c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.rand(10 ,4, requires_grad = True)\n",
    "b1 = torch.rand(10, requires_grad = True)\n",
    "\n",
    "W2 = torch.rand(10, 10, requires_grad = True)\n",
    "b2 = torch.rand(10, requires_grad = True)\n",
    "\n",
    "W3 = torch.rand(3, 10, requires_grad = True)\n",
    "b3 = torch.rand(3, requires_grad = True)\n",
    "\n",
    "def loss(prediction, y_one_hot):\n",
    "    return -torch.sum(y_one_hot * torch.log(prediction)) \n",
    "\n",
    "def forward(x):\n",
    "    z1 = W1 @ x + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = torch.sigmoid(z2)\n",
    "    scores = W3 @ a2 + b3    \n",
    "    print(scores)\n",
    "    probs = torch.softmax(scores,-1)\n",
    "    return probs\n",
    "\n",
    "def step(x,y): \n",
    "    probs = forward(x)\n",
    "    loss_val = loss(probs, y)\n",
    "    loss_val.backward()  # THIS DOES THE BACKPROP FOR US :-)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936f07e-525c-46a7-9319-40300036a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "data_x = torch.tensor([   # 4 attributes \n",
    "    [0.5, -0.2, 0.1, 0.4],\n",
    "    [1.5,  0.2, 1.1, -0.4],\n",
    "    [0.3,  0.8, 0.5, 0.7],\n",
    "    [0.6,  0.3, -0.9, 1.0],\n",
    "    [1.0, -0.1, 0.2, -0.3]\n",
    "])\n",
    "\n",
    "# There are 3 classes 0, 1, 2.\n",
    "data_y = torch.tensor([0, 2, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265717fb-d344-42b6-9e34-81d937b69007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y): \n",
    "    return torch.nn.functional.one_hot(y, num_classes=3)\n",
    "\n",
    "step(data_x[0], one_hot(data_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc8579-9af9-4f3f-bdae-0a269208849c",
   "metadata": {},
   "source": [
    "### The Module Abstraction \n",
    "\n",
    "The `torch.nn` component contains a number of tools to build neural networks more easily. Rather than building neural networks from scratch using individual tensors, which can become very cumbersome for larger networks, we can construct the network from `Modules`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26125349-7ce3-4fe5-a090-da909cfc253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # common convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6200b86-eeb2-483a-9540-d718d6839f2e",
   "metadata": {},
   "source": [
    "`nn.Module` is the base class for all other Modules. We can get a standard neural network layer using the `nn.Sequential` Module. \n",
    "\n",
    "Each Module has a `.forward(x)` method that returns the activation of the module when applied to the tensor `x`. Alternatively, the module can just be called *as if it was a function*, which will implicitly call `.forward`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a7a0e-f3b5-4aa2-9cb8-a99904b97a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(4, 3)  #linear layer without an activation function.  \n",
    "test_input = data_x[0]\n",
    "linear(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e1923-5333-43e0-a252-3baf8b22c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.forward(test_input) # equivalent, but directly calling the module is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070ecb9-6b28-4c97-828a-81c2dc20106b",
   "metadata": {},
   "source": [
    "Each module keeps track of its parameters (weights and biases). All of these have `requires_grad=True` set by default. But the beauty of the module abstraction is that we rarely have to look at these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744f07d-bbd9-4496-a1fa-10ad143b5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in linear.parameters(): \n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c89c41-c1ab-4751-9ba4-15bfe3d540e3",
   "metadata": {},
   "source": [
    "**Writing your own Modules**\n",
    "\n",
    "Modules can be nested inside of other modules -- this is fairly common in larger neural networks, and also the way you typically write your own neural network in pyTorch.\n",
    "\n",
    "The basic approach is to inherit from nn.Module (review the notes on inheritance in the Python review section -- but really all this means is that the methods of nn.Module become available in your custom module). We add all component modules in the `__init__` method, which is called when the new class is first instantiated. Then we write a `forward` method to define the computation flow through the network. autograd will construct the computation graph for us in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adc306-f94d-46b5-b3a0-4e73eab6d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F # anoter common convention\n",
    "\n",
    "class Model(nn.Module): # inherit from the nn.Module class\n",
    "\n",
    "    def __init__(self): \n",
    "        # This is the constructor. \n",
    "        super().__init__() # call constructor of the parent class\n",
    "        \n",
    "        # Specify all the component modules inside of __init__. \n",
    "        self.hidden1 = nn.Linear(4,10)\n",
    "        self.hidden2 = nn.Linear(10,10)\n",
    "        self.out_layer = nn.Linear(10,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Need to implement the forward method to specify computation flow\n",
    "        z1 = self.hidden1(x)\n",
    "        a1 = F.relu(z1)\n",
    "        z2 = self.hidden2(a1)\n",
    "        a2 = F.relu(z2) \n",
    "        z3 = self.out_layer(a2) \n",
    "        return z3 \n",
    "\n",
    "\n",
    "# Note: We are NOT applying softmax here. Below, we will use pyTorch's CategoricalCrossentropy loss, which implicitly computes the softmax \n",
    "# before comparing to the target one-hot vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7387430-8292-4c5e-b198-6f0201f2ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model(data_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e01119-912b-4e9c-8f9a-9c2e56471f8b",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We can now write a training loop, either for our hand-built neural network or for the the Module version.  \n",
    "We will use <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\">nn.CrossEntropyLoss</a>. This implementation allows us to compare the prediction directly to an integer class. So the target can just be an an integer scalar, not a one-hot vector. CrossEntropyLoss also computes the softmax operation implicitly before comparing to the target. \n",
    "\n",
    "PyTorch provides a number of differen optimization algorithms. Here we will just use the basic Stochastic Gradient Descent algorithm we already know. But more advanced algorithms are available in the <a href=\"https://docs.pytorch.org/docs/stable/optim.html\">torch.optim</a> package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09cab2-3a25-409d-a5f2-51a36867c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_x, data_y, epochs=100, learning_rate=0.1):\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "    \n",
    "    #register the model's parameters with the optimizer, so it knows what to update. \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "    model.train() # Put the model in training mode. This doesn't hve any effect for now, but it's good practice. \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i in range(len(data_x)):\n",
    "            x = data_x[i]                \n",
    "            y = data_y[i]\n",
    "\n",
    "            prediction = model(x)            # raw class scores            \n",
    "\n",
    "            loss = loss_fn(prediction, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11831a-d37b-4797-bd39-1c3e826c1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, data_x, data_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ea05b-7320-41c0-885f-4af90413c27d",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "We can write another function to evaluate the model on some test data. \n",
    "\n",
    "Note that we are turning off gradient computation here, because otherwise pyTorch would automatically store all activations during the forward pass and compute the gradients. Turning this off saves time and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1bb50-f680-40af-aa7a-dd5fa7b2ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_x, data_y):\n",
    "    correct = 0\n",
    "    \n",
    "    model.eval()  # place the model in evaluation mode -- again this doesn't have an effect for now, but is good practice. \n",
    "    \n",
    "    with torch.no_grad():  # turn off automatic gradient computation and storage -- not needed since we are no longer training. \n",
    "        for i in range(data_x.shape[0]):\n",
    "            x = data_x[i]\n",
    "            y = data_y[i]\n",
    "\n",
    "            prediction = model(x)                 # raw class scores. We are just interested in the max, so no need to use softmax.\n",
    "            predicted = torch.argmax(prediction)  # predicted class index -- the one with the max score.\n",
    "\n",
    "            if predicted == y:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / data_x.shape[0]\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c8601-2f7f-4e03-93f7-92c4e76117a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, data_x, data_y) # Run it on the training data for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374419f9-6356-425d-b282-de437909e932",
   "metadata": {},
   "source": [
    "### Datasets and Batching\n",
    "\n",
    "\n",
    "***Mini Batches***\n",
    "So far, we have trained the model on a single input/output pair at a time: present the input, compute the loss, compute the gradients and update the weights. This process can be slow with large amounts of training data, and it also tends to be unstable as a single example can drastically change the weights. One important trick is to shuffle the data so in each epoch they are presented in different order (the \"stochastic\" in SGD). \n",
    "\n",
    "Alternatively, we could use the entire available training data for the forward pass, average the losses (to compute the training error) and compute the gradients for this aggregate error. But with large datasets this can result in overfitting and can be quite memory intensive.\n",
    "\n",
    "As a compromise, we often present the data in \"mini batches\": perform the forward pass on a few data items at a time, average the losses for those items, then compute the gradient. This is especially useful on GPUs, where we can efficiently parallelize tensor operations and \"stack\" together the forward and backward computation for multiple data items at the same time. \n",
    "\n",
    "***Dataset and Dataloader***\n",
    "pyTorch provides a mechanism for maintaining data sets and automatically creating batches for training. \n",
    "\n",
    "The class `torch.utils.data.Dataset` is a base class that can be implemented by various concrete Dataset implementations. It has two methods: \n",
    "\n",
    "* __len__(self) — returns the number of samples in the dataset. Enables the `len(data)` method.\n",
    "* __getitem__(self, k) — returns an (input, output) tuple for the data at index k. Enables indexing, such as `data[5]`.\n",
    "\n",
    "We can build our own Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671224b4-30c3-408c-81ac-624d425f7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummyData(Dataset): \n",
    "\n",
    "    def __init__(self):\n",
    "        # Possibly load the data from a file and store it in a data structure. Here we will just use the dummy data from above\n",
    "        # Dummy data\n",
    "        self.data_x = torch.tensor([   # 4 attributes \n",
    "            [0.5, -0.2, 0.1, 0.4],\n",
    "            [1.5,  0.2, 1.1, -0.4],\n",
    "            [0.3,  0.8, 0.5, 0.7],\n",
    "            [0.6,  0.3, -0.9, 1.0],\n",
    "            [1.0, -0.1, 0.2, -0.3]])\n",
    "        \n",
    "        self.data_y = torch.tensor([0, 2, 1, 1, 0])\n",
    "        \n",
    "    \n",
    "    def __len__(self): \n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self,k): \n",
    "        return (self.data_x[k], self.data_y[k])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b5fbd-c034-487c-a9f0-e72f01ee25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DummyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf7b03-ec0c-4126-8662-78c6b4ecfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bde565-f98d-4a4b-946c-134884fa6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d904e5-4618-4599-a16d-f8f9c481f804",
   "metadata": {},
   "source": [
    "The DataLoader will automatically batch the data and allow us to obtain one item at a time for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3c3e8-5580-435b-a150-372aaa82cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18812485-2885-4036-bdba-aef127e74f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader: \n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77754368-40ba-4853-946e-6552ad22a8ce",
   "metadata": {},
   "source": [
    "If the data is in a tensor, PyTorch actually provides an existing TensorDataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dfaee-94a8-4bb0-a6d4-176aa5ab281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "data = TensorDataset(data_x, data_y)\n",
    "loader = DataLoader(data, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557b143-0822-45dc-a7af-080eee11de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d612d8-d34d-4a3b-a83a-56130436b802",
   "metadata": {},
   "source": [
    "Now update the train method to use the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb95f4f-96c5-4ddb-96c4-a2fa84101479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model() # Reset the model\n",
    "\n",
    "def train(model, dataloader, epochs=100, learning_rate=0.1):\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss() \n",
    "    \n",
    "    #register the model's parameters with the optimizer, so it knows what to update. \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "    model.train() # Put the model in training mode. This doesn't hve any effect for now, but it's good practice. \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x,y in dataloader:            \n",
    "\n",
    "            prediction = model(x)            # raw class scores            \n",
    "\n",
    "            loss = loss_fn(prediction, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, loader)  # This now uses batches! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c10755-b85c-4198-bd1d-3f5ec306b7de",
   "metadata": {},
   "source": [
    "### GPU Acceleration \n",
    "\n",
    "Graphics Processing Units (GPUs) are specialized processors originally developed to accelerate graphics rendering and animation, such as those used in video games and CGI for films.\n",
    "\n",
    "GPUs are are designed for parallel processing: They can apply the same operation across many data elements simultaneously. This makes them  well-suited for the kinds of computations common in neural networks, such as element-wise operations and matrix multiplication.\n",
    "\n",
    "PyTorch can used Nvidia's CUDA computing library behind the scenes to accellerate tensor operations. There are also backends for other GPU frameworks, such as Apple's MPS (Metal Performance Shaders) for their Silicon architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377be87-5bc9-4776-a1d6-46693baf825b",
   "metadata": {},
   "source": [
    "First, we can check if CUDA is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f647ee-46d2-405b-a933-fafbf29317ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314a807-8abb-467c-b13c-990662c1882c",
   "metadata": {},
   "source": [
    "All we need to do in order for operations to be performed by the GPU is to make sure the relevant tensors (data and weights) are moved to the GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af74a56-2178-400f-88bd-f4e9d0e1613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE) # place all parameters of the model, including gradients and activations into GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22e1ac",
   "metadata": {},
   "source": [
    "We also need to move the training data to the GPU by changing the Dataset. Often we have more RAM than GPU memory, so it's a good idea to only move the items returned by `__getitem__` to the GPU when working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad180011-9bba-4ab7-bb25-ee65396dc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyData(Dataset): \n",
    "\n",
    "    def __init__(self):\n",
    "        # Possibly load the data from a file and store it in a data structure. Here we will just use the dummy data from above\n",
    "        # Dummy data\n",
    "        self.data_x = torch.tensor([   # 4 attributes \n",
    "            [0.5, -0.2, 0.1, 0.4],\n",
    "            [1.5,  0.2, 1.1, -0.4],\n",
    "            [0.3,  0.8, 0.5, 0.7],\n",
    "            [0.6,  0.3, -0.9, 1.0],\n",
    "            [1.0, -0.1, 0.2, -0.3]])\n",
    "        \n",
    "        self.data_y = torch.tensor([0, 2, 1, 1, 0])\n",
    "        \n",
    "    \n",
    "    def __len__(self): \n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self,k): \n",
    "        return (self.data_x[k].to(DEVICE), self.data_y[k].to(DEVICE))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DummyData()\n",
    "loader = DataLoader(data, batch_size = 2, shuffle = True)\n",
    "\n",
    "train(model, loader) # this will now run on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054479c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
