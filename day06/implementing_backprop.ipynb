{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 6 - Implementing Backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an implementation of the neural network from last week. You can either use the implementation below or your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Dummy data\n",
    "data_x = np.array([   # 4 attributes \n",
    "    [0.5, -0.2, 0.1, 0.4],\n",
    "    [1.5,  0.2, 1.1, -0.4],\n",
    "    [0.3,  0.8, 0.5, 0.7],\n",
    "    [0.6,  0.3, -0.9, 1.0],\n",
    "    [1.0, -0.1, 0.2, -0.3]\n",
    "])\n",
    "\n",
    "# There are 3 classes 0, 1, 2.\n",
    "data_y = np.array([0, 2, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of the dummy data is that it will allow us to test our implementation. We should be able to learn the training data perfectly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a neural network with 4 inputs, 3 output units (softmax activated), and 2 hidden layers (10 units each). The hidden layers should use the sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "W1 = np.random.rand(10,4) * 0.01   # we make the initial weights smaller\n",
    "b1 = np.random.rand(10) * 0.01\n",
    "\n",
    "W2 = np.random.rand(10,10) * 0.01\n",
    "b2 = np.random.rand(10) * 0.01\n",
    "\n",
    "# I forgot to provide these in the initial notebook -- with two hidden layers you need the parameters for \n",
    "# each hidden layer PLUS the parameters for the output layer.\n",
    "W3 = np.random.rand(3, 10) * 0.01\n",
    "b3 = np.random.rand(3) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))    # Good to use the numpy methods for numeric stability\n",
    "\n",
    "def softmax(z):\n",
    "    # First compute the denominator\n",
    "    denom = np.sum(np.exp(z))\n",
    "    # then element-wise divide e^z by the denominator\n",
    "    return np.exp(z) / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the activation functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zvec = np.array([0.9, 0.1, 0.7, 0.4])\n",
    "probs = softmax(zvec)\n",
    "print(probs)\n",
    "print(np.sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass for a single input vector\n",
    "def forward(x):\n",
    "    z1 = np.matmul(W1, x) + b1 \n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = np.matmul(W2,a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.matmul(W3,a2) + b3\n",
    "    out = softmax(z3)\n",
    "    \n",
    "    return out  # returns probabilities over 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(data_x):\n",
    "    probs = forward(x)\n",
    "    pred_class = np.argmax(probs)\n",
    "    print(f\"Example {i}: softmax: {probs}, predicted:{pred_class}, target:{data_y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also implement accuracy to measure performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data_x, data_y):\n",
    "    correct = 0\n",
    "    for x, y in zip(data_x, data_y):\n",
    "        pred = np.argmax(forward(x))\n",
    "        if pred == y:\n",
    "            correct += 1\n",
    "    return correct / len(data_y)\n",
    "\n",
    "print(\"Accuracy on dummy data:\", accuracy(data_x, data_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Backward Pass\n",
    "\n",
    "The loss function we will use is negative log-likelihood / categorical cross-entropy $loss(y',y)=- \\sum_{i=1}^d y_{(i)} \\log y'$ Where y' is the the prediction and y is the target. Note that in a classification problem the target is a one-hot vector, so only one of the dimensions are included in the sum (the one for which the one-hot target is 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(prediction, y_one_hot): \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to compute a one-hot vector from the integer class label for the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(class_label, num_classes):\n",
    "    \n",
    "    \n",
    "# one_hot(data_y[0],3)  # test it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss(forward(data_x[0]), one_hot(data_y[0],3))  #and test the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the derivative of the sigmoid function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(z):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the backward step -- this is for a single input vector and target class. Because we need to keep track of the forward activations (and pre-activations), we will just duplicate the forward pass code below. \n",
    "\n",
    "Alternatively, we could call the forward function above and then return the activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x, target, learning_rate = 0.1):\n",
    "        \n",
    "    global W1, b1, W2, b2, W3, b3  #ensure that parameter changes survive outside the scope of this function\n",
    "    \n",
    "    # Forward pass\n",
    "    z1 = W1 @ x + b1   # might want to use the @ notation instead of .matmul for readability\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = W3 @ a2 + b3\n",
    "    out = softmax(z3)\n",
    "    \n",
    "    \n",
    "    y = one_hot(target,3)\n",
    "    loss_val = loss(out, y)\n",
    "    \n",
    "\n",
    "    # Backward pass\n",
    "    \n",
    "    # *** Output layer ***\n",
    "    #deltaL = ...\n",
    "    \n",
    "    # weight update\n",
    "    #dW3 = ...             # you need deltaL and a2 and you need to reshape them into a column vector and a row vector. \n",
    "    #db3 = ...\n",
    "    \n",
    "\n",
    "    # pass back \n",
    "    # da2 = ...           # compute da2 from deltaL\n",
    "    # delta2 = ..         # then compute delta2 from da2 (note: use the derivative of the sigmoid)\n",
    "    \n",
    "    # *** Hidden layer 2 ***\n",
    "    # weight update\n",
    "    #dW2 = ...\n",
    "    #db2 = ...\n",
    "    \n",
    "    #pass back\n",
    "    #da1 = ...\n",
    "    #delta1 = ...\n",
    "    \n",
    "    \n",
    "    # *** Hidden layer 1 ***\n",
    "    # weight update\n",
    "    #dW1 = ...\n",
    "    #db1 = ...\n",
    "    \n",
    "    \n",
    "    # It's customary to actually update the weights in the end, once the gradients are computed\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3    \n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    \n",
    "    return loss_val # we will return the loss so we can see it decrease "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run an epoch -- a single pass over the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(data_x, data_y, lr =0.1):\n",
    "    total_loss = 0\n",
    "    for x, y in zip(data_x, data_y):\n",
    "        total_loss += step(x,y, lr)\n",
    "    print(f\"Epoch loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train the model (for 100 epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1 # i found that you need to set the learning rate higher to fit the data set\n",
    "\n",
    "for i in range(100):    \n",
    "   epoch(data_x, data_y, learning_rate)\n",
    "   print(accuracy(data_x, data_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, you should be able to learn the dummy data _perfectly_. The resulting model will be overfitted, but we have demonstrated that we are learning. \n",
    "\n",
    "Optional next step: Train a model for the penguin data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
