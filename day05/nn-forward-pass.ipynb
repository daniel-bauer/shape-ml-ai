{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning and Artificial Intelligence \n",
    "Summer High School Academic Program for Engineers (2025)\n",
    "## Day 4 - Implementing a Neural Network using Numpy (forward pass only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will construct a neural network for classifying the penguin data. However, you will _not_ train the neural network, only define the topology. Next week, we will discuss how to implement training. \n",
    "\n",
    "Note also that we will later use off-the-shelf implementations that are much more efficient (e.g. they use GPU accelleration). The point of this exercise is to make sure you understand the basic bulding blocks of a neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data set \n",
    "Before working with the penguin data, you can use the following dummy dataset: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([   # 4 attributes \n",
    "    [0.5, -0.2, 0.1, 0.4],\n",
    "    [1.5,  0.2, 1.1, -0.4],\n",
    "    [0.3,  0.8, 0.5, 0.7],\n",
    "    [0.6,  0.3, -0.9, 1.0],\n",
    "    [1.0, -0.1, 0.2, -0.3]\n",
    "])\n",
    "\n",
    "# There are 3 classes 0, 1, 2. We won't actually use the labels yet.\n",
    "data_y = np.array([0, 2, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Define a neural network with 4 inputs, 3 output units (softmax activated), and 2 hidden layers (10 units each). The hidden layers should use the sigmoid activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the weight matrices and bias vectors\n",
    "\n",
    "#W1 = ...\n",
    "#b1 = ...\n",
    "#W2 = ...\n",
    "#b1 = ...\n",
    "\n",
    "# look into the numpy.random.rand function for random initialization \n",
    "\n",
    "# define activation functions \n",
    "\n",
    "def sigmoid(z): \n",
    "    return\n",
    "\n",
    "def softmax(z): # note z is a vector\n",
    "    return \n",
    "\n",
    "def forward(input_x):\n",
    "    # This is where the main code defining the neural network goes, including the matrix multiplication part. \n",
    "    \n",
    "    return ... # Return the predicted class OR alternatively a probability distribution over classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your neural network on a number of training examples. \n",
    "Then load the penguin training data (see day 2) and try your neural network on that data. \n",
    "\n",
    "Stretch goal 1) define an error function that computes the accuracy of your predictions on the entire test data. Think about what kind of error function you could use if the foward() function outputs a probability distribution rather than a single class. \n",
    "\n",
    "Stretch goal 2) right now the forward function works only with a single vector input. Make sure that it works with an input matrix (i.e. a batch of data stacked together). It should output a batch of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
