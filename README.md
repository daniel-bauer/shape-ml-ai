### Welcome to SHAPE Session 2, Machine Learning & Artificial Intelligence

AI systems like ChatGPT and other language models are becoming a big part of our everyday lives. But how do these systems actually work? What can they do, and where do they fall short?

This crash course in machine learning and AI will introduce you to the basics. You’ll start with simpler machine learning models, learn about neural networks, and then explore how advanced models like ChatGPT are built and trained.

**Who is this course for?** Students who are comfortable with basic coding in any language (like Java or Python).

#### Teaching Staff

Instructor: [Daniel Bauer](http://www.cs.columbia.edu/~bauer)  
Summer Student Leaders: Batu Yeltekin, Michael Del Toro, Leo Zubarev

**Location:** **Botwinick Lab - Mudd 1224**


#### **What you Will Learn**

*   A quick review of Python
*   Important math concepts: basic calculus and working with vectors and matrices
*   How to handle different types of data, including tables, images, and text
*   How neural networks work and learn from data
*   How to classify images using Convolutional Neural Networks
*   How to represent words in a way that computers understand
*   How transformers (a type of ML model) work
*   The basics of models like GPT
*   How pretraining and fine-tuning make these models smarter
*   How to interact with language models using prompts and API calls

**Class Format:** Each day, we’ll have short lectures and work on real hands-on projects in small groups.  

A typical day:

| Time | Session |
| --- | --- |
| 9:00 AM | Check-in |
| 9:15 AM | Morning session (workshops, speakers, or project time) |
| 10:15  <br>AM | Professor-led class and lab time |
| 12:15 PM | Lunch Break |
| 1:30 PM | Afternoon check-in |
| 1:45  <br>PM | Professor-led class and project time (SSLs available, Professor Bauer available until 3pm) |
| 4:00 PM | Elective or Community Hour |
| 5:00 PM | Dismissal |

The following schedule is tentative and subject to change: 

|     |     |     |
| --- | --- | --- |
| **Date** | **Topic/Materials** | **Exercises/Projects  <br>** |
| Mon 7/28 | Welcome and Introduction / Course overview. <br><br>Python review if necessary. | [Codio setup](/courses/229803/pages/codio-setup-guide "Codio setup guide")<br><br>Game playing warmup challenge |
| Tue 7/29 | [Introduction to Supervised Machine Learning](https://github.com/daniel-bauer/shape-ml-ai/blob/main/01-intro/01-intro_supervised_ml.ipynb) | Loading and exploring a tabular dataset with pandas and matplotlib  <br>  <br>Training a classifier with scikit-learn |
| Wed 7/30 | Biological and artificial neurons. Linear models and the Perceptron algorithm. | Perceptron by hand.<br><br>Implement the Perceptron algorithm and run it on a linearly classifiable tabular dataset. Visualize the separation boundary. |
| Thu 7/31 | Vectors and matrices in numpy.<br><br>Vector and matrix multiplication.<br><br>Neural Network basics (forward activation only). Computation graphs and activation functions. | Build a mini neural network from scratch in numpy (manual forward pass only). |
| Fri 8/1 | Neural Networks continued.<br><br>Optimization using gradient descent for a single unit.<br><br>  <br>Multi-variable calculus basics. | Train a neural network in pytorch (using autograd this time) on the penguin data. Write a suitable Dataset class and train() function. |
| Mon 8/4 | Chain rule. Backpropagation. | Optional: Extend the mini neural net from Thursday to include backprop.  <br>  <br>Alternatively: use pytorch to build a NN to classify the MNIST data (handwritten digit recognition) |
| Tue 8/5 | wrap up basic NN implementation and MNIST project. |     |
| Wed 8/6 | Convolutional Neural Networks and Image Classification | Build a CNN in pyTorch and train it on the CIFAR 100 data. |
| Thu 8/7 |     | image classifier continued. Try it on photos of new objects!  <br>  <br>(maybe use chatGPT to write code for preprocessing and a simple web interface to upload images?) |
| Fri 8/8 | Text in neural networks. Word embeddings. Word2Vec. | Implement and train a word2vec skipgram model on a small dataset. Visualize word similarities. |
| Mon 8/11 | Explanation of attention and transformers. BERT and GPT. | Build a simple neural network language model. |
| Tue 8/12 | Fine-tuning and instruction tuning. Large Language Models and prompting. | Fine-tune a model on a classification task (sentiment)? |
| Wed 8/13 | Retrieval Augmented Generation? | Final LLM Project |
| Thu 8/14 | Final Project | Final Project |
| Fri  8/15 | Final Project | Final Project |

