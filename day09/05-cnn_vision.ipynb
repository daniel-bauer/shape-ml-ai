{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5530e64-f017-4dbd-a72f-13b391deeb07",
   "metadata": {},
   "source": [
    "## Machine Learning and Artificial Intelligence \n",
    "Summer High School Academic Program for Engineers (2025)\n",
    "## Convolutional Neural Networks and Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d64426-b6dc-4620-88c3-23b1e4e26eaa",
   "metadata": {},
   "source": [
    "###### Image Classification and the Cifar datasets\n",
    "\n",
    "We will work on an an image classification task: Given an image (provided in the matplotlib format you are already familiar with), your goal is to identify the central object depicted, choosing from a set of possible object labels.\n",
    "\n",
    "The Cifar dataset contains 60000 images at 32x32 resolution with 3 color channels, each with a label. Labels are available in coarse form (Cifar10 - 10 total labels) and more fine-grained form (Cifar100 - 100 total labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e099ba9-ae27-43b1-8f29-ebf55cdd15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "# Load data -- adjust the path as needed\n",
    "train_dict = unpickle('cifar-100-python/train')\n",
    "test_dict = unpickle('cifar-100-python/test')\n",
    "\n",
    "# Extract images and labels\n",
    "X_train = train_dict['data']  # shape: (50000, 3072)\n",
    "y_train = train_dict['fine_labels']  # or 'coarse_labels'\n",
    "\n",
    "X_test = test_dict['data']\n",
    "y_test = test_dict['fine_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e358e5-efd5-4fa2-9639-ae635303d12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0255c684-473f-4e53-a506-4568140f5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = unpickle('cifar-100-python/meta')\n",
    "label_names = meta['fine_label_names']  # List of 100 class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376e1353-643d-4e92-9085-55fefe35df70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x789eda267410>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dict['data'][99].reshape(3,32,32).transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d61a1cd-2d02-445c-a7a0-475a6c9f25e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'turtle'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names[train_dict['fine_labels'][99]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278c427-dc6a-497e-b028-0055e8798e1c",
   "metadata": {},
   "source": [
    "Note that image data is very different from the sort of tabular data we worked with so far. In tabular data, relevant feature have already been extracted through **manual feature engineering**. In image data (and text data -- see next week), we do not have an explicit set of features and we use neural networks to automaticically extract features. This is known as **representation learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc6648-adf1-4de5-b3da-95609ee55274",
   "metadata": {},
   "source": [
    "Let's say we want to train a standard feed forward neural network to classify these images. Each image has 32*32=1024 pixels. There are three color channels, so we get (3,32,32) tensors representing each image -- a total of 3072 input units. \n",
    "\n",
    "Every unit in the first hidden layer will have to be connected to each of these input units. A single hidden layer of size 100 would therefore need 10*3072 = 307,200 parameters!\n",
    "\n",
    "CNNs offer a solution to this problem by learning to transform images into lower-dimensional representations that retain relevant information for classification. In CNN vision models, images are represented as a combination of increasingly abstract features that may appear in different local regions. \n",
    "\n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/cnn_feature_hierarchy.png\" width=600px>\n",
    "\n",
    "When building a feed-forward network for MNIST, you may have realized that the number of parameters quickly becomes very large. You could train such a model on modern GPUs. Historically, one of the first CNNs was LeNet5 (LeCun et al, 1998), which was applied to MNIST. At that time, GPUs didn't exist and automatic feature extraction was a novel idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ad46d-e28c-4558-bb03-6ac3bfeb05e8",
   "metadata": {},
   "source": [
    "### Convolution in 1D\n",
    "\n",
    "Assume we have an input sequence such as the following (maybe a time series, such as a sequence of daily stock prices). This is different from the sort of tabular feature-based representations we have seen so far.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "2017c008-608a-473d-8de3-59a17e7553eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([4, 5, 1, 6, 8]) # size 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ba286-ac44-4e8e-a0d7-b4ab063b6dab",
   "metadata": {},
   "source": [
    "Assume we create a feed-forward layer of size (4,). This would require a (5x4) weight matrix, that is 20 total parameters (plus the bias).\n",
    "\n",
    "Now assume instead that we have a \"**kernel**\" (also called a **filter**), which is a matrix of size (2,). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b35a3e66-ef2c-4af8-af77-bda19479119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = torch.tensor([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43707f4-201a-4f33-b864-f1e5cda8d7f1",
   "metadata": {},
   "source": [
    "We now slide the kernel over the input, computing a dot product in each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3c568718-b681-44d5-a419-46d140886574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2] @ kernel   # [4,5] @ [-1,1]^T = 4*-1 + 5*1 = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "94036538-a44c-45cc-8c4a-b2a781a21f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:3] @ kernel # [5,1] @ [-1, 1]^T = 5*-1 + 1*1 = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c51422a5-a380-4e13-9d05-1d69c41c7108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2:4] @ kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "125c3232-5931-4059-bd1b-e36e52269f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3:5] @ kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde82adc-cc44-4d23-8126-3ce3926ec275",
   "metadata": {},
   "source": [
    "We then put the results back into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "557234f2-b12a-4660-b4ba-80b8d9a517f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, -4,  5,  2])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.tensor([x[i:i+2] @ kernel for i in range(len(x)-1)])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d6382-669a-47b3-a028-8a1f12f96d69",
   "metadata": {},
   "source": [
    "This operation is called a **convolution** of the input (side note: in mathematical terms this operation is more correctly called cross-correlation, but in ML people use the term convolution). The output is often called a **feature map**.\n",
    "\n",
    "The idea is that the kernel can pick up occurences of a certain pattern/feature in the input. In this example, the feature will match any region of the input that show large changes between consecutive values: -4 indicates a steep drop from 5 to 1, while 5 indicates an indcrease from 1 to 6. \n",
    "\n",
    "The result is a size (5,) representation of the input that can be used, for example, for a classification task (for instance, maybe many sharp changes suggest volatility and thus indicate that we should sell the stock). The kernel is a weight vector that is learned while we train the model on the classification task. \n",
    "\n",
    "Note that convolution requires far fewer parameters, i.e. the size of the filter(2 features instead of 20!). It does however make an assumption that a good representation for the object can be obtained from the local features. \n",
    "\n",
    "In an actual implementation, the filter does not actually \"slide\" the filter over the input step by step. Instead, the filter is applied to all input positions simultaneously using batching, so this operation can be parallelized on a GPU. There really are only 2 weights and they are shared between all the input positions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54678b33-3301-4f44-8dd4-aa9743372bea",
   "metadata": {},
   "source": [
    "### 2D Convolution Applied to Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ca31e93-dbbe-4b54-9c81-485ebae12c4d",
   "metadata": {},
   "source": [
    "Recall that we can represent an images as a (width,height,color_channels) tensor. \n",
    "\n",
    "<img src=\"http://www.cs.columbia.edu/~bauer/shape/image_representation.png\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22715dd8-6fd4-41c6-9719-de5d5601a78d",
   "metadata": {},
   "source": [
    "#### Convolution on a single 2D Matrix\n",
    "\n",
    "Let's consider a single color channel first. We can define a filter in 2D. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0310ffb-afd5-4baf-9b4a-e23ab4f5961b",
   "metadata": {},
   "source": [
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/kernel_illustration1.png\" width=\"300px\"> <img src=\"https://www.cs.columbia.edu/~bauer/shape/kernel_illustration2.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f684b-b17c-4fd3-b534-5263d617aab9",
   "metadata": {},
   "source": [
    "As before, the filter is applied to all possible positions in the image. For each position we calculate the kernel activation as the weighted sum between the entries in the two matrices. For example, for the upper left position: $1 \\cdot 234 + 0 \\cdot 0 + 0 \\cdot 0 + 2\\cdot 230 = 694$. For the second position (left corner of the kernel matches row 1 column 2 of the input matrix): $1 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 230 + 2 \\cdot 7 = 140$\n",
    "\n",
    "There are 3x3=9 positions, so the resulting feature map is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "469fbf9c-5e31-4ce1-8293-a73127c2652c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[694, 140,   0],\n",
       "        [  4, 684,  70],\n",
       "        [  4,   8, 517]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[694, 140,   0],\n",
    "              [  4, 684,  70],\n",
    "              [  4,   8, 517]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af02b47-5ee2-48a5-96b6-1644f2ae458c",
   "metadata": {},
   "source": [
    "**Stride and Padding**\n",
    "\n",
    "When defining a convolution operation we can specify the stride and padding. \n",
    "\n",
    "**Stride** is the number of indices that the kernel \"slides\" in each step. In the example above, the stride is 1. If we set stride = 2, we move the kernel by 2 positions instead, skipping every other position.\n",
    "\n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/kernel_illustration3.png\" width=\"400px\"> \n",
    "\n",
    "Increasing the stride will result in future entries in the feature map. With stride=2, the feature map on a (4,4) matrix will be (2,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f58ec609-d03a-4eef-903f-a5b30545026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[694,   0],\n",
       "        [  4, 517]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[694,0], \n",
    "              [4,517]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6beed-5149-437e-b60f-69c6d911c987",
   "metadata": {},
   "source": [
    "**Padding** Padding allows the kernel to reach beyond the boundaries of the input matrix. Typically the matrix is padded with 0. \n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/kernel_illustration4.png\" width=\"350px\"> \n",
    "\n",
    "With padding, the size of the feature map is identical to the size of the input matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab9db0-6a1d-4125-82d9-fbd53432f700",
   "metadata": {},
   "source": [
    "#### Multiple Kernels\n",
    "\n",
    "We often apply multiple kernels at the same time, and then stack the resulting feature maps together into a 3D tensor. The idea is that each kernel will learn to focus on a different feature of the image. The different kernels are called \"out channels\" in pyTorch and the number of kernels is sometimes referred to as the \"depth\".\n",
    "\n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/kernel_illustration5.png\" width=\"400px\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b892d57-f2ae-4441-b647-a9c1e2b7b6bd",
   "metadata": {},
   "source": [
    "In the example, we have two kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "160cf809-b45b-41f2-bc99-f4c95f5de3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  0.],\n",
       "         [ 0.,  2.]],\n",
       "\n",
       "        [[ 0.,  3.],\n",
       "         [-1.,  0.]]])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[1,0],[0,2]],[[0,3],[-1,0]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece8271-3ca6-41a3-b5c2-8f26822d51e6",
   "metadata": {},
   "source": [
    "The input matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "43cfb058-882b-4b11-ae02-f01c704b1e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[234.,   0.,   0.,   0.],\n",
       "        [  0., 230.,  70.,   0.],\n",
       "        [  0.,   2., 227.,   0.],\n",
       "        [  0.,   2.,   3., 145.]])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[234,0,0,0],[0,230,70,0],[0,2,227,0],[0,2,3,145]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd50688-1477-4bc3-bc98-3653579ada78",
   "metadata": {},
   "source": [
    "Applying both kernels results in the tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "e7480fcc-ca2d-405c-9fcb-73e4eb437567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 694.,  140.,    0.],\n",
       "         [   4.,  684.,   70.],\n",
       "         [   4.,    8.,  517.]],\n",
       "\n",
       "        [[   0., -230.,  -70.],\n",
       "         [ 690.,  208., -227.],\n",
       "         [   6.,  679.,   -3.]]])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[694., 140.,   0.],\n",
    "               [  4., 684.,  70.],\n",
    "               [  4.,   8., 517.]],\n",
    "\n",
    "               [[   0., -230.,  -70.],\n",
    "                [ 690.,  208., -227.],\n",
    "                [  6.,  679.,   -3.]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add71185-dd53-4563-a6ea-0dd61103145a",
   "metadata": {},
   "source": [
    "Each kernel represents an abstract features, and the feature maps measure to what degree each feature appears in different local regions of the image. Visualizing the feature maps makes this intuition clear. \n",
    "\n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/feature_map_visualization.png\" width=\"500px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815405a1-dade-4bcb-a539-a0c6a79e10dd",
   "metadata": {},
   "source": [
    "#### Multiple Input Channels\n",
    "\n",
    "If we are working on images, we have 3 color channels (RGB). Similarly, if we have multiple convolution layers in a neural network, then subsequent layers will need to process the multiple \"out channels\" from the previous layer. \n",
    "\n",
    "Assume we just have a single kernel and three input channels. The kernel is applied to each channel individually, resulting in three different feature maps. Then, the resulting maps are summed pointwise, resulting in a single feature map. \n",
    "\n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/kernel_illustration6.png\" width=\"500px\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b62ef1-b3d9-41d8-a8da-f61794f31912",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "Pooling reduces the dimensionality of a feature map (or stack of feature maps). The goal is to reduce the number of parameters, while maintainign the most relevant information in the feature map. In addition to saving memory it may also help prevent overfitting by generalizing over small differences in the image.\n",
    "\n",
    "Pooling is essentially a down-sampling method. We divide the feature map up into smaller (typically square) windows which may or may not overlap. As in convolution, this can be controlled using the stride parameter. Each of these sections results in a single value. We can either take the max value in the section (**max pooling**) or average the values (**value pooling**). \n",
    "\n",
    "For example, max pooling on a (4,4) feature map with window size 2x2 and stride 2 might look like this: \n",
    "\n",
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/pooling.png\" width=\"500px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca65eff-9249-4391-a2e9-9c17856a1496",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "CNN vision models use Convolution and Pooling to learn increasingly abstract feature representations of an input image. Here is a (very basic) CNN vision model for the Cifar-10 task: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65589d1b-981a-4246-bc15-ef5546c43cc7",
   "metadata": {},
   "source": [
    "<img src=\"https://www.cs.columbia.edu/~bauer/shape/cnn_architecture.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecd9e7-f81d-4428-a36f-a812e80e7606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
